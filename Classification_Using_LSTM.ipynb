{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification Using LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmWekRTzUgHa"
      },
      "source": [
        "# **Sequence Classification using LSTM**\r\n",
        "\r\n",
        "*Problem Description:*\r\n",
        "The problem that we will use to demonstrate sequence learning in this tutorial is the IMDB movie review sentiment classification problem. Each movie review is a variable sequence of words and the sentiment of each movie review must be classified.\r\n",
        "\r\n",
        "\r\n",
        "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment.\r\n",
        "\r\n",
        "The data was collected by Stanford researchers and was used in a 2011 paper where a split of 50-50 of the data was used for training and test. An accuracy of 88.89% was achieved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BiFQwLnPcfK"
      },
      "source": [
        "import numpy\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.preprocessing import sequence\r\n",
        "# fix random seed for reproducibility\r\n",
        "numpy.random.seed(7)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk1Oe8rWVEsa"
      },
      "source": [
        "Keras provides access to the IMDB dataset built-in. The imdb.load_data() function allows you to load the dataset in a format that is ready for use in neural network and deep learning models.\r\n",
        "\r\n",
        "\r\n",
        "The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMMfAdAwPiJU",
        "outputId": "f5e74ce6-6aa6-458d-fc55-54952c45fb3d"
      },
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\r\n",
        "top_words = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoDlBGqOPibY"
      },
      "source": [
        "# truncate and pad input sequences\r\n",
        "max_review_length = 500 ## Constrain the sequence length for each movie review.\r\n",
        "\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDAhBr-3Vd8_"
      },
      "source": [
        "We will map each movie review into a real vector domain, a popular technique when working with text called word embedding. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\r\n",
        "\r\n",
        "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\r\n",
        "\r\n",
        "We will map each word onto a 32 length real valued vector. We will also limit the total number of words that we are interested in modeling to the 5000 most frequent words, and zero out the rest.\r\n",
        "\r\n",
        "Now that we have defined our problem and how the data will be prepared and modeled, we are ready to develop an LSTM model to classify the sentiment of movie reviews.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Because it is a binary classification problem, *log loss* is used as the loss function (binary_crossentropy in Keras). The efficient ADAM optimization algorithm is used. The model is fit for only 2 epochs because it quickly overfits the problem (Avoided using dropout). A large batch size of 64 reviews is used to space out weight updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP8gmNh-PqZ4",
        "outputId": "d3624919-fb6b-458d-c83c-80e75dcbfc01"
      },
      "source": [
        "# create the model\r\n",
        "embedding_vecor_length = 32\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\r\n",
        "model.add(LSTM(100))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "print(model.summary())\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 262s 669ms/step - loss: 0.4773 - accuracy: 0.7740 - val_loss: 0.3770 - val_accuracy: 0.8365\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 263s 673ms/step - loss: 0.3078 - accuracy: 0.8758 - val_loss: 0.3485 - val_accuracy: 0.8530\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 262s 669ms/step - loss: 0.2667 - accuracy: 0.8965 - val_loss: 0.3278 - val_accuracy: 0.8617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa0130806a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD1_EFkRPvih",
        "outputId": "9f8f4816-a3c5-48dc-bb21-f4aa76453c1b"
      },
      "source": [
        "# Final evaluation of the model\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 86.17%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}