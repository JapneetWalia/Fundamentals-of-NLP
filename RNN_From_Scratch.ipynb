{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_From_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCb1oOs9pL2T"
      },
      "source": [
        "# **Recurrent Neural Network - Implementation from Scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx61tL-Ykf7e"
      },
      "source": [
        "## Dependencies\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import sys\r\n",
        "from datetime import datetime\r\n",
        "import pickle\r\n",
        "import tqdm\r\n",
        "import random\r\n",
        "from pathlib import Path"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKyFaXoNqZyU"
      },
      "source": [
        "##RNN\n",
        "\n",
        "\n",
        "### Activation Functions\n",
        "\n",
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def backward(self, x):\n",
        "        return (1.0 - x) * x\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        exp_scores = np.exp(x)\n",
        "        return exp_scores / np.sum(exp_scores)\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        return -np.log(x[y])\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        x[y] -= 1.0\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Encapsulating the characteristics of a layer. Functions defining the forward pass and backward propagation mechanisms.\n",
        "\n",
        "class Layer:\n",
        "  \n",
        "    def forward(self, x, prev_s, u, w, v):\n",
        "        \"\"\"\n",
        "        x : input array\n",
        "        prev_s : array\n",
        "        u,v,w : weight matrices\n",
        "\n",
        "        \"\"\"\n",
        "        activation = Sigmoid()\n",
        "        output = Softmax()\n",
        "        self.mulu = np.matmul(x, u)\n",
        "        self.mulw = np.matmul(prev_s, w)\n",
        "        self.sin = np.add(self.mulu, self.mulw)\n",
        "        self.sout = activation.forward(self.sin)\n",
        "        self.oin = np.matmul(self.sout, v)\n",
        "        self.oout = output.forward(self.oin)\n",
        "\n",
        "    def backward(self, x, prev_s, y, u, w, v):\n",
        "        \"\"\"\n",
        "        y : integer\n",
        "        \"\"\"\n",
        "        activation = Sigmoid()\n",
        "        output = Softmax()\n",
        "        self.loss = output.loss(self.oout, y)\n",
        "        self.dldoi = output.backward(self.oout, y)\n",
        "        self.doidso = v\n",
        "        self.doidv = self.sout\n",
        "        self.dsodsi = activation.backward(self.sout)\n",
        "        self.dsidu = x\n",
        "        self.dsidpso = w\n",
        "        self.dsidw = prev_s\n",
        "\n",
        "\n",
        "\n",
        "### Functions defining the forward pass and backward propagation mechanisms\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (word_dim, hidden_dim))\n",
        "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n",
        "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, word_dim))\n",
        "        self.layers = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x : array of integers (denoting one training example i.e. a sentence)\n",
        "        \"\"\"\n",
        "        T = len(x)\n",
        "        self.layers = []\n",
        "        prev_s = np.zeros(self.hidden_dim)\n",
        "        for t in range(T):\n",
        "            layer = Layer()\n",
        "            input = np.zeros(self.word_dim)\n",
        "            input[x[t]]=1\n",
        "            layer.forward(input, prev_s, self.U, self.W, self.V)\n",
        "            prev_s = layer.sout\n",
        "            self.layers.append(layer)\n",
        "    \n",
        "    def generate(self, seed, num=100, k=10):\n",
        "        \"\"\"\n",
        "        seed - one character (tokenized) - integer\n",
        "        num - number of characters to generate\n",
        "        \"\"\"\n",
        "        # print(detokenize(seed))\n",
        "        text = []\n",
        "        text.append(seed)\n",
        "        prev_s = np.zeros(self.hidden_dim)\n",
        "        for i in range(num):\n",
        "            layer = Layer()\n",
        "            input = np.zeros(self.word_dim)\n",
        "            input[seed]=1\n",
        "            layer.forward(input, prev_s, self.U, self.W, self.V)\n",
        "            prev_s = layer.sout\n",
        "            # seed = np.argmax(layer.oout)\n",
        "            # print(detokenize(seed))\n",
        "            # select randomly from 75% and above\n",
        "            temp = sorted(layer.oout, reverse=True)\n",
        "            threshold = temp[k-1]\n",
        "            top = [index for index, val in enumerate(layer.oout) if val>=threshold]\n",
        "            seed = random.choice(top)\n",
        "            # print(layer.oout[seed])\n",
        "            text.append(seed)\n",
        "        return text\n",
        "\n",
        "    def load(self, filename):\n",
        "        myfile = Path(filename)\n",
        "        if(myfile.exists):\n",
        "            print('Loading weights...')\n",
        "            with open(filename,'rb') as f:\n",
        "                self.U, self.W, self.V = pickle.load(f)\n",
        "\n",
        "    # def total_loss(self):\n",
        "    #     loss  = 0\n",
        "    #     for layer in self.layers:\n",
        "    #         loss += layer.loss\n",
        "    #     return loss\n",
        "\n",
        "    def calculate_total_loss(self, x, y):\n",
        "        L = 0\n",
        "        # For each sentence...\n",
        "        for i in np.arange(len(y)):\n",
        "            self.forward(x[i])\n",
        "            # We only care about our prediction of the \"correct\" words\n",
        "            correct_word_predictions = [j.oout[y[i][k]] for k,j in enumerate(self.layers)]\n",
        "            # Add to the loss based on how off we were\n",
        "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
        "        return L\n",
        " \n",
        "    def calculate_loss(self, x, y):\n",
        "        # Divide the total loss by the number of training examples\n",
        "        N = np.sum((len(y_i) for y_i in y))\n",
        "        return self.calculate_total_loss(x,y)/N\n",
        "\n",
        "    def calculate_grads(self, x, y):\n",
        "        for t, layer in enumerate(self.layers):\n",
        "            input = np.zeros(self.word_dim)\n",
        "            input[x[t]]=1\n",
        "            prev_s = np.zeros(self.hidden_dim)\n",
        "            layer.backward(input, prev_s, y[t], self.U, self.W, self.V)\n",
        "            prev_s = layer.sout\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        \"\"\"\n",
        "        x,y: array of integers\n",
        "      \n",
        "        \"\"\"\n",
        "        self.forward(x)\n",
        "        self.calculate_grads(x, y)\n",
        "        dldu = np.zeros(self.U.shape)\n",
        "        dldw = np.zeros(self.W.shape)\n",
        "        dldv = np.zeros(self.V.shape)\n",
        "        T = len(self.layers)\n",
        "        for t in np.arange(T)[::-1]:\n",
        "            dldv += np.outer(self.layers[t].doidv,self.layers[t].dldoi) # dim: [hidden_dim * word_dim]\n",
        "            delta_t = np.matmul(self.layers[t].doidso, self.layers[t].dldoi) # dEt/dSt(out) dim: [hidden_dim] \n",
        "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
        "                delta_t = delta_t * self.layers[bptt_step].dsodsi # dim: [hidden_dim]\n",
        "                dldw += np.outer(self.layers[bptt_step].dsidw, delta_t)\n",
        "                dldu += np.outer(self.layers[bptt_step].dsidu, delta_t)\n",
        "                delta_t = np.matmul(self.layers[bptt_step].dsidpso, delta_t) # dim: [hidden_dim]\n",
        "        return (dldu, dldw, dldv)\n",
        "\n",
        "    def sgd_step(self, x, y, learning_rate):\n",
        "        dU, dW, dV = self.backward(x, y)\n",
        "        self.U -= learning_rate * dU\n",
        "        self.V -= learning_rate * dV\n",
        "        self.W -= learning_rate * dW\n",
        "\n",
        "    def train(self, X, Y, learning_rate, nepoch, evaluate_loss_after):\n",
        "        num_examples_seen = 0\n",
        "        losses = []\n",
        "        print(\"\\n\\n TRAINING STARTED \\n\\n\")\n",
        "        for epoch in range(nepoch):\n",
        "            if (epoch % evaluate_loss_after == 0):\n",
        "                    loss = self.calculate_loss(X,Y)\n",
        "                    losses.append((num_examples_seen, loss))\n",
        "                    time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                    print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
        "                    # Adjust the learning rate if loss increases\n",
        "                    if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
        "                        learning_rate = learning_rate * 0.5\n",
        "                        print(\"Setting learning rate to %f\" % learning_rate)\n",
        "                    sys.stdout.flush()\n",
        "            # For each training example...\n",
        "            for i in tqdm.tqdm(range(len(Y))):\n",
        "                # print(i, )\n",
        "                # X[i] and Y[i] are one training example\n",
        "                self.sgd_step(X[i], Y[i], learning_rate)\n",
        "                num_examples_seen += 1\n",
        "            with open('uwv.pkl', 'wb') as f:  \n",
        "                pickle.dump([self.U, self.W, self.V], f)\n",
        "            f.close()\n",
        "                \n",
        "        \n",
        "        return losses"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WpP95pauU8s"
      },
      "source": [
        "## Toenizers\n",
        "\n",
        "def tokenize(x):\n",
        "    if(ord(x)>=ord('a') and ord(x)<=ord('z')):\n",
        "        return ord(x)-97\n",
        "    elif(ord(x)>=ord('A') and ord(x)<=ord('Z')):\n",
        "        return ord(x)-39\n",
        "    elif(x == ' '):\n",
        "        return 52\n",
        "    elif(x == '.'):\n",
        "        return 53\n",
        "    elif(x == ','):\n",
        "        return 54\n",
        "    elif(x == \"'\"):\n",
        "        return 55\n",
        "    elif(x == '?'):\n",
        "        return 56\n",
        "    elif(x == '!'):\n",
        "        return 57\n",
        "    elif(x == ';'):\n",
        "        return 58\n",
        "    elif(x == \":\"):\n",
        "        return 59\n",
        "    elif(x == '\\n'):\n",
        "        return 60\n",
        "    else:\n",
        "        return \"UNKNOWN\"\n",
        "\n",
        "def detokenize(x):\n",
        "    if(x>=0 and x<=25):\n",
        "        return chr(x+97)\n",
        "    elif(x>=26 and x<=51):\n",
        "        return chr(x+39)\n",
        "    elif(x==52):\n",
        "        return ' '\n",
        "    elif(x==53):\n",
        "        return '.'\n",
        "    elif(x==54):\n",
        "        return ','\n",
        "    elif(x==55):\n",
        "        return \"'\"\n",
        "    elif(x==56):\n",
        "        return '?'\n",
        "    elif(x==57):\n",
        "        return '!'\n",
        "    elif(x==58):\n",
        "        return ';'\n",
        "    elif(x==59):\n",
        "        return ':'\n",
        "    elif(x==60):\n",
        "        return '\\n'\n",
        "    else:\n",
        "        return \"UNKNOWN\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8o0npZwsE_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22faa8d3-9adf-44da-fb17-812c16c7165b"
      },
      "source": [
        "## TRAINING\n",
        "\n",
        "f = open('input.txt', 'r')\n",
        "sentences = list(filter(None, f.read().split('\\n\\n')))\n",
        "f.close()\n",
        "# print((sentences))\n",
        "tokens = []\n",
        "X = []\n",
        "Y = []\n",
        "# word_dim = 58\n",
        "for i in range(len(sentences)):\n",
        "    tokens.append([])\n",
        "    for j in range(len(sentences[i])):\n",
        "        if(tokenize(sentences[i][j])!=\"UNKNOWN\"):\n",
        "            tokens[i].append(tokenize(sentences[i][j]))\n",
        "    X.append(tokens[i][:-1])\n",
        "    Y.append(tokens[i][1:])\n",
        "\n",
        "model = RNN(61, 200, 10)\n",
        "model.load('uwv.pkl')\n",
        "model.train(X, Y, 0.03, 10, 1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading weights...\n",
            "\n",
            "\n",
            " TRAINING STARTED \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:145: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 10:50:12: Loss after num_examples_seen=0 epoch=0: 2.388640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [07:04<00:00,  1.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 10:57:27: Loss after num_examples_seen=786 epoch=1: 2.573664\n",
            "Setting learning rate to 0.015000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [07:06<00:00,  1.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 11:04:44: Loss after num_examples_seen=1572 epoch=2: 2.322710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [06:20<00:00,  2.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 11:11:14: Loss after num_examples_seen=2358 epoch=3: 2.303991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [06:15<00:00,  2.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 11:17:40: Loss after num_examples_seen=3144 epoch=4: 2.295098\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [06:26<00:00,  2.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 11:24:17: Loss after num_examples_seen=3930 epoch=5: 2.289515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [06:08<00:00,  2.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 11:30:35: Loss after num_examples_seen=4716 epoch=6: 2.284626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [06:37<00:00,  1.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 11:37:23: Loss after num_examples_seen=5502 epoch=7: 2.280435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [07:04<00:00,  1.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 11:44:38: Loss after num_examples_seen=6288 epoch=8: 2.276772\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [07:08<00:00,  1.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-14 11:51:58: Loss after num_examples_seen=7074 epoch=9: 2.273430\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 786/786 [07:03<00:00,  1.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 2.3886398643810924),\n",
              " (786, 2.5736644796278756),\n",
              " (1572, 2.3227095533399758),\n",
              " (2358, 2.3039911005625555),\n",
              " (3144, 2.2950976430649783),\n",
              " (3930, 2.2895149757699875),\n",
              " (4716, 2.2846261271956716),\n",
              " (5502, 2.2804346217307367),\n",
              " (6288, 2.2767715289260777),\n",
              " (7074, 2.2734298334607974)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tGLZ8prsNjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eaac351-8ce1-423b-8679-25c547d8e4ed"
      },
      "source": [
        "##GENERATE\n",
        "\n",
        "\n",
        "model = RNN(61,200)\n",
        "model.load('uwv.pkl')\n",
        "\n",
        "# print(model.U.shape)\n",
        "# generate(start_token, num_of_chars, top_k random predictions)\n",
        "text = model.generate(tokenize('T'), 300, 3)\n",
        "\n",
        "for i in text:\n",
        "    print(detokenize(i), end=\"\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading weights...\n",
            "Thered ater te of arge on took out ter of\n",
            "ecte at onedited on and ter to diojuter ar our of\n",
            "en andot on of ande ount toness.o\n",
            "diojudeder atichounbece terg th anceres thiss..  oure terede to toojess. Founbeding\n",
            "weles outin at tooks,\n",
            "Ser tedion ton of to sionestinbured artiountiojut out ourge on the ou"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}