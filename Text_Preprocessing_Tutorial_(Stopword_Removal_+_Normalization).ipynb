{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing Tutorial (Stopword Removal + Normalization).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aVAW_j1xrWi"
      },
      "source": [
        "## **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm768QOji2Pa"
      },
      "source": [
        "## **Stopwords and Stopword removal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRBwil3si3fY"
      },
      "source": [
        "Stopwords are simply the most common words in a language like “we” and “are”. These words probably do not help at all in NLP tasks such as sentiment analysis or text classifications. Hence, we can remove stopwords to save computing time and efforts in processing large volumes of text.\r\n",
        "\r\n",
        "\r\n",
        "Removing stopwords is not a hard and fast rule in NLP. It depends upon the task that we are working on. For tasks like text classification, where the text is to be classified into different categories, stopwords are removed or excluded from the given text so that more focus can be given to those words which define the meaning of the text.\r\n",
        "\r\n",
        "\r\n",
        "For instance, words like 'wood', 'book', and 'table' add more meaning to the text as compared to the words 'is' and 'on'.\r\n",
        "\r\n",
        "Following are the sample code for stopword removal using the two most popular libraries in NLP - spacy and nltk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUj1nEUJgkvt"
      },
      "source": [
        "## *Using nltk*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OffIMG9mf13P",
        "outputId": "ac93fa2b-5ed9-43d2-9a55-b2937ee2afd1"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6RanbOcf9UG",
        "outputId": "de146635-8134-4e5a-e705-93146d5da47a"
      },
      "source": [
        "from nltk.corpus import stopwords  \r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import word_tokenize  \r\n",
        "  \r\n",
        "example_sent = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \r\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \r\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\r\n",
        "  \r\n",
        "stop_words = set(stopwords.words('english'))  \r\n",
        "  \r\n",
        "word_tokens = word_tokenize(example_sent)  \r\n",
        "  \r\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]  \r\n",
        "  \r\n",
        "filtered_sentence = []  \r\n",
        "  \r\n",
        "for w in word_tokens:  \r\n",
        "    if w not in stop_words:  \r\n",
        "        filtered_sentence.append(w)  \r\n",
        "  \r\n",
        "print(word_tokens)  \r\n",
        "print(filtered_sentence)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['He', 'determined', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', 'his', 'claims', 'to', 'the', 'wood-cuting', 'and', 'fishery', 'rihgts', 'at', 'once', '.', 'He', 'was', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'rights', 'had', 'become', 'much', 'less', 'valuable', ',', 'and', 'he', 'had', 'indeed', 'the', 'vaguest', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were', '.']\n",
            "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRpePomkh72W"
      },
      "source": [
        "## *Using spaCy*\r\n",
        "\r\n",
        "spaCy is one of the most versatile and widely used libraries in NLP. We can quickly and efficiently remove stopwords from the given text using SpaCy. It has a list of its own stopwords that can be imported as STOP_WORDS from the spacy.lang.en.stop_words class.\r\n",
        "\r\n",
        "In our case, we used spaCy’s inbuilt stopwords, but we should be cautious and modify the stopwords list accordingly. E.g., for sentiment analysis, the word “not” is important in the meaning of a text such as “not good”. However, spaCy included “not” as a stopword. We therefore modify the stopwords by the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c0H0x-MgCOg",
        "outputId": "cfe3e9dd-ffe3-4833-941e-ac077681ada0"
      },
      "source": [
        "from spacy.lang.en import English\r\n",
        "\r\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\r\n",
        "nlp = English()\r\n",
        "\r\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \r\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \r\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\r\n",
        "\r\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\r\n",
        "my_doc = nlp(text)\r\n",
        "\r\n",
        "# Create list of word tokens\r\n",
        "token_list = []\r\n",
        "for token in my_doc:\r\n",
        "    token_list.append(token.text)\r\n",
        "\r\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
        "\r\n",
        "# Create list of word tokens after removing stopwords\r\n",
        "filtered_sentence =[] \r\n",
        "\r\n",
        "for word in token_list:\r\n",
        "    lexeme = nlp.vocab[word]\r\n",
        "    if lexeme.is_stop == False:\r\n",
        "        filtered_sentence.append(word) \r\n",
        "print(token_list)\r\n",
        "print(filtered_sentence)   "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'determined', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', 'his', 'claims', 'to', 'the', 'wood', '-', 'cuting', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', 'He', 'was', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'rights', 'had', 'become', 'much', 'less', 'valuable', ',', 'and', 'he', 'had', '\\n', 'indeed', 'the', 'vaguest', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were', '.']\n",
            "['determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood', '-', 'cuting', '\\n', 'fishery', 'rihgts', '.', 'ready', 'becuase', 'rights', 'valuable', ',', '\\n', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OET3fpJOqSQF"
      },
      "source": [
        "# **Text Normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT-SHb3Iqqex"
      },
      "source": [
        "# Stemming\r\n",
        "\r\n",
        "Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word\r\n",
        "\r\n",
        "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sNtj476vF2N",
        "outputId": "2ed007d3-0c57-4d97-a5b0-ae7278f7fce3"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "\r\n",
        "set(stopwords.words('english'))\r\n",
        "\r\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \r\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \r\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english')) \r\n",
        "  \r\n",
        "word_tokens = word_tokenize(text) \r\n",
        "    \r\n",
        "filtered_sentence = [] \r\n",
        "  \r\n",
        "for w in word_tokens: \r\n",
        "    if w not in stop_words: \r\n",
        "        filtered_sentence.append(w) \r\n",
        "\r\n",
        "Stem_words = []\r\n",
        "ps =PorterStemmer()\r\n",
        "for w in filtered_sentence:\r\n",
        "    rootWord=ps.stem(w)\r\n",
        "    Stem_words.append(rootWord)\r\n",
        "print(filtered_sentence)\r\n",
        "print(Stem_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
            "['He', 'determin', 'drop', 'litig', 'monastri', ',', 'relinguish', 'claim', 'wood-cut', 'fisheri', 'rihgt', '.', 'He', 'readi', 'becuas', 'right', 'becom', 'much', 'less', 'valuabl', ',', 'inde', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1uyG7480qCK"
      },
      "source": [
        "**NOTE:** Stemming is a crude heuristic that chops the ends off of words and hence, the result may not be good or actual words. E.g., stemming “caring” will result in “car”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW1nZfCBpkSb"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuLaSU0dpygP"
      },
      "source": [
        "Lemmatization is the process of converting a word to its base form, e.g., “caring” to “care”. We use spaCy’s lemmatizer to obtain the lemma, or base form, of the words. Other variant from nltk library is also a popular option.\r\n",
        "\r\n",
        "Stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. For example, if you lemmatize the word 'Caring', it would return 'Care'. If you stem, it would return 'Car' and this is erroneous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-MWHIN0wHT8",
        "outputId": "364ba1e3-3415-46ef-b4ff-e99f493f776e"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "nltk.download('wordnet')\r\n",
        "import nltk\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "set(stopwords.words('english'))\r\n",
        "\r\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \r\n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \r\n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english')) \r\n",
        "  \r\n",
        "word_tokens = word_tokenize(text) \r\n",
        "    \r\n",
        "filtered_sentence = [] \r\n",
        "  \r\n",
        "for w in word_tokens: \r\n",
        "    if w not in stop_words: \r\n",
        "        filtered_sentence.append(w) \r\n",
        "print(filtered_sentence) \r\n",
        "\r\n",
        "lemma_word = []\r\n",
        "import nltk\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "for w in filtered_sentence:\r\n",
        "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\r\n",
        "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\r\n",
        "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\r\n",
        "    lemma_word.append(word3)\r\n",
        "print(lemma_word)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
            "['He', 'determine', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claim', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'right', 'become', 'much', 'le', 'valuable', ',', 'indeed', 'vague', 'idea', 'wood', 'river', 'question', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJNmDM0z1nDM"
      },
      "source": [
        "**A bit on nltk vs spaCy:**\r\n",
        "\r\n",
        "1. NLTK provides a plethora of algorithms to choose from for a particular problem which is boon for a researcher but a bane for a developer. Whereas, spaCy keeps the best algorithm for a problem in its toolkit and keep it updated as state of the art improves.\r\n",
        "\r\n",
        "2. NLTK supports various languages whereas spaCy have statistical models for 7 languages (English, German, Spanish, French, Portuguese, Italian, and Dutch). It also supports named entities for multi language.\r\n",
        "\r\n",
        "3. NLTK is a string processing library. It takes strings as input and returns strings or lists of strings as output. Whereas, spaCy uses object-oriented approach. When we parse a text, spaCy returns document object whose words and sentences are objects themselves.\r\n",
        "\r\n",
        "4. spaCy has support for word vectors whereas NLTK does not.\r\n",
        "\r\n",
        "5. As spaCy uses the latest and best algorithms, its performance is usually good as compared to NLTK. As we can see below, in word tokenization and POS-tagging spaCy performs better, but in sentence tokenization, NLTK outperforms spaCy. Its poor performance in sentence tokenization is a result of differing approaches: NLTK attempts to split the text into sentences. In contrast, spaCy constructs a syntactic tree for each sentence, a more robust method that yields much more information about the text."
      ]
    }
  ]
}